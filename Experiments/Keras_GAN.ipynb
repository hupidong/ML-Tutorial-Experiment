{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/hupidong/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/hupidong/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/hupidong/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/hupidong/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/hupidong/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/hupidong/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个训练过程可以说判别器 D 和生成器 G 对价值函数 V(G,D) 进行了极小极大化博弈：\n",
    "![gan_tf_keras1.png](./Res/gan_tf_keras1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最优生成器\n",
    "\n",
    "因为最优的判别器D(x)=P_data/(P_data+P_G)，我们将其代入V（G,D）可得：\n",
    "![gan_tf_keras2.png](./Res/gan_tf_keras2.png)\n",
    "该积分进行变换得：\n",
    "![gan_tf_keras3.png](./Res/gan_tf_keras3.png)\n",
    "假设存在两个分布 P 和 Q，且这两个分布的平均分布 M=(P+Q)/2，那么这两个分布之间的 JS 散度为 P 与 M 之间的 KL 散度加上 Q 与 M 之间的 KL 散度再除以 2；因此可化为：\n",
    "![gan_tf_keras4.png](./Res/gan_tf_keras4.png)\n",
    "\n",
    "JS 散度的取值为 0 到 log2。若两个分布完全没有交集，那么 JS 散度取最大值 log2；若两个分布完全一样，那么 JS 散度取最小值 0。当 P_G=P_data 时，JSD(P_data||P_G) 为 0。综上所述，生成分布当且仅当等于真实数据分布式时，我们可以取得最优生成器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    #下面搭建生成器的架构，首先导入序贯模型（sequential），即多个网络层的线性堆叠\n",
    "    model = Sequential()\n",
    "    #添加一个全连接层，输入为100维向量，输出为1024维\n",
    "    model.add(Dense(input_dim=100, units=1024))\n",
    "    #添加一个激活函数tanh\n",
    "    model.add(Activation('tanh'))\n",
    "    #添加一个全连接层，输出为128×7×7维度\n",
    "    model.add(Dense(128*7*7))\n",
    "    #添加一个批量归一化层，该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    #Reshape层用来将输入shape转换为特定的shape，将含有128*7*7个元素的向量转化为7×7×128张量\n",
    "    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,)))\n",
    "    #2维上采样层，即将数据的行和列分别重复2次\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    #添加一个2维卷积层，卷积核大小为5×5，激活函数为tanh，共64个卷积核，并采用padding以保持图像尺寸不变\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    #卷积核设为1即输出图像的维度\n",
    "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最优判别器\n",
    "\n",
    "原论文中价值函数可写为在 x 上的积分，即将数学期望展开为积分形式：\n",
    "![gan_tf_keras5.png](./Res/gan_tf_keras5.png)\n",
    "其实求积分的最大值可以转化为求被积函数的最大值。而求被积函数的最大值是为了求得最优判别器 D，因此不涉及判别器的项都可以看作为常数项。\n",
    "若令判别器 D(x) 等于 y，那么被积函数可以写为：\n",
    "![gan_tf_keras6.png](./Res/gan_tf_keras6.png)\n",
    "为了找到最优的极值点，如果 a+b≠0，我们可以用以下一阶导求解：\n",
    "![gan_tf_keras6.png](./Res/gan_tf_keras7.png)\n",
    "因此，最优判别器D(x)=P_data/(P_data+P_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    #下面搭建判别器架构，同样采用序贯模型\n",
    "    model = Sequential()\n",
    "    \n",
    "    #添加2维卷积层，卷积核大小为5×5，激活函数为tanh，输入shape在‘channels_first’模式下为（samples,channels，rows，cols）\n",
    "    #在‘channels_last’模式下为（samples,rows,cols,channels），输出为64维\n",
    "    model.add(\n",
    "            Conv2D(64, (5, 5),\n",
    "            padding='same',\n",
    "            input_shape=(28, 28, 1))\n",
    "            )\n",
    "    model.add(Activation('tanh'))\n",
    "    #为空域信号施加最大值池化，pool_size取（2，2）代表使图片在两个维度上均变为原长的一半\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (5, 5)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #Flatten层把多维输入一维化，常用在从卷积层到全连接层的过渡\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    #一个结点进行二值分类，并采用sigmoid函数的输出作为概念\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(g, d):\n",
    "    #将前面定义的生成器架构和判别器架构组拼接成一个大的神经网络，用于判别生成的图片\n",
    "    model = Sequential()\n",
    "    #先添加生成器架构，再令d不可训练，即固定d\n",
    "    #因此在给定d的情况下训练生成器，即通过将生成的结果投入到判别器进行辨别而优化生成器\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    #生成图片拼接\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[:, :, 0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对于每一次迭代：\n",
    "\n",
    "-  从真实数据分布 P_data 抽取 m 个样本\n",
    "-  从先验分布 P_prior(z) 抽取 m 个噪声样本\n",
    "-  将噪声样本投入 G 而生成数据，即x^tilde = G(Z^i)；通过最大化 V 的近似而更新判别器参数θ_d\n",
    "\n",
    "以上是学习判别器 D 的过程。因为学习 D 的过程是计算 JS 散度的过程，并且我们希望能最大化价值函数，所以该步骤会重复 k 次。\n",
    "\n",
    "-  从先验分布 P_prior(z) 中抽取另外 m 个噪声样本 {z^1,...,z^m}\n",
    "-  通过极小化 V^tilde 而更新生成器参数θ_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(BATCH_SIZE):\n",
    "    \n",
    "    # 国内好像不能直接导入数据集，我们试了几次都不行，后来将数据集下载到本地'~/.keras/datasets/'，也就是当前目录（我的是用户文件夹下）下的.keras文件夹中。\n",
    "    #下载的地址为：https://s3.amazonaws.com/img-datasets/mnist.npz\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    #iamge_data_format选择\"channels_last\"或\"channels_first\"，该选项指定了Keras将要使用的维度顺序。\n",
    "    #\"channels_first\"假定2D数据的维度顺序为(channels, rows, cols)，3D数据的维度顺序为(channels, conv_dim1, conv_dim2, conv_dim3)\n",
    "    \n",
    "    #转换字段类型，并将数据导入变量中\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train[:, :, :, None]\n",
    "    X_test = X_test[:, :, :, None]\n",
    "    # X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:])\n",
    "    \n",
    "    #将定义好的模型架构赋值给特定的变量\n",
    "    d = discriminator_model()\n",
    "    g = generator_model()\n",
    "    d_on_g = generator_containing_discriminator(g, d)\n",
    "    \n",
    "    #定义生成器模型判别器模型更新所使用的优化算法及超参数\n",
    "    d_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    #编译三个神经网络并设置损失函数和优化算法，其中损失函数都是用的是二元分类交叉熵函数。编译是用来配置模型学习过程的\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    \n",
    "    #前一个架构在固定判别器的情况下训练了生成器，所以在训练判别器之前先要设定其为可训练。\n",
    "    d.trainable = True\n",
    "    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    \n",
    "    #下面在满足epoch条件下进行训练\n",
    "    for epoch in range(30):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        \n",
    "        #计算一个epoch所需要的迭代数量，即训练样本数除批量大小数的值取整；其中shape[0]就是读取矩阵第一维度的长度\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        \n",
    "        #在一个epoch内进行迭代训练\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            \n",
    "            #随机生成的噪声服从均匀分布，且采样下界为-1、采样上界为1，输出BATCH_SIZE×100个样本；即抽取一个批量的随机样本\n",
    "            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n",
    "            \n",
    "            #抽取一个批量的真实图片\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            \n",
    "            #生成的图片使用生成器对随机噪声进行推断；verbose为日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录\n",
    "            generated_images = g.predict(noise, verbose=1)\n",
    "            \n",
    "            #每经过100次迭代输出一张生成的图片\n",
    "            if index % 100 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(\n",
    "                    \"./GAN/\"+str(epoch)+\"_\"+str(index)+\".png\")\n",
    "            \n",
    "            #将真实的图片和生成的图片以多维数组的形式拼接在一起，真实图片在上，生成图片在下\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            \n",
    "            #生成图片真假标签，即一个包含两倍批量大小的列表；前一个批量大小都是1，代表真实图片，后一个批量大小都是0，代表伪造图片\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            \n",
    "            #判别器的损失；在一个batch的数据上进行一次参数更新\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            \n",
    "            #随机生成的噪声服从均匀分布\n",
    "            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "            \n",
    "            #固定判别器\n",
    "            d.trainable = False\n",
    "            \n",
    "            #计算生成器损失；在一个batch的数据上进行一次参数更新\n",
    "            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "            \n",
    "            #令判别器可训练\n",
    "            d.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            \n",
    "            #每100次迭代保存一次生成器和判别器的权重\n",
    "            if index % 100 == 9:\n",
    "                g.save_weights('generator', True)\n",
    "                d.save_weights('discriminator', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## my own test code\n",
    "(X_train,y_train),(X_test,y_test)=mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "X_train=X_train[:,:,:,None]\n",
    "g=generator_model()\n",
    "g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "batch_size=128\n",
    "noise=np.random.uniform(-1,1,size=(batch_size,100))\n",
    "generated_images=g.predict(noise)\n",
    "image_batch=X_train[0:batch_size]\n",
    "X=np.concatenate((image_batch,generated_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k(BATCH_SIZE,k):\n",
    "    \n",
    "    # 国内好像不能直接导入数据集，我们试了几次都不行，后来将数据集下载到本地'~/.keras/datasets/'，也就是当前目录（我的是用户文件夹下）下的.keras文件夹中。\n",
    "    #下载的地址为：https://s3.amazonaws.com/img-datasets/mnist.npz\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    #iamge_data_format选择\"channels_last\"或\"channels_first\"，该选项指定了Keras将要使用的维度顺序。\n",
    "    #\"channels_first\"假定2D数据的维度顺序为(channels, rows, cols)，3D数据的维度顺序为(channels, conv_dim1, conv_dim2, conv_dim3)\n",
    "    \n",
    "    #转换字段类型，并将数据导入变量中\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train[:, :, :, None]\n",
    "    X_test = X_test[:, :, :, None]\n",
    "    # X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:])\n",
    "    \n",
    "    #将定义好的模型架构赋值给特定的变量\n",
    "    d = discriminator_model()\n",
    "    g = generator_model()\n",
    "    d_on_g = generator_containing_discriminator(g, d)\n",
    "    \n",
    "    #定义生成器模型判别器模型更新所使用的优化算法及超参数\n",
    "    d_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    #编译三个神经网络并设置损失函数和优化算法，其中损失函数都是用的是二元分类交叉熵函数。编译是用来配置模型学习过程的\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    \n",
    "    #前一个架构在固定判别器的情况下训练了生成器，所以在训练判别器之前先要设定其为可训练。\n",
    "    d.trainable = True\n",
    "    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    \n",
    "    #下面在满足epoch条件下进行训练\n",
    "    for epoch in range(30):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        \n",
    "        #计算一个epoch所需要的迭代数量，即训练样本数除批量大小数的值取整；其中shape[0]就是读取矩阵第一维度的长度\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        \n",
    "        #在一个epoch内进行迭代训练\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            \n",
    "            #随机生成的噪声服从均匀分布，且采样下界为-1、采样上界为1，输出BATCH_SIZE×100个样本；即抽取一个批量的随机样本\n",
    "            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n",
    "            \n",
    "            #抽取一个批量的真实图片\n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            \n",
    "            #生成的图片使用生成器对随机噪声进行推断；verbose为日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录\n",
    "            generated_images = g.predict(noise, verbose=1)\n",
    "            \n",
    "            #每经过100次迭代输出一张生成的图片\n",
    "            if index % 100 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(\n",
    "                    \"./GAN/\"+str(epoch)+\"_\"+str(index)+\".png\")\n",
    "            \n",
    "            #将真实的图片和生成的图片以多维数组的形式拼接在一起，真实图片在上，生成图片在下\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            \n",
    "            #生成图片真假标签，即一个包含两倍批量大小的列表；前一个批量大小都是1，代表真实图片，后一个批量大小都是0，代表伪造图片\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n",
    "            \n",
    "            #判别器的损失；在一个batch的数据上进行一次参数更新\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "            #print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            \n",
    "            if index % k == 0: \n",
    "                #随机生成的噪声服从均匀分布\n",
    "                noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "                #固定判别器\n",
    "                d.trainable = False\n",
    "            \n",
    "                #计算生成器损失；在一个batch的数据上进行一次参数更新\n",
    "                g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "            \n",
    "                #令判别器可训练\n",
    "                d.trainable = True\n",
    "                print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "                print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            \n",
    "            #每100次迭代保存一次生成器和判别器的权重\n",
    "            if index % 100 == 9:\n",
    "                g.save_weights('generator', True)\n",
    "                d.save_weights('discriminator', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(BATCH_SIZE, nice= False ):\n",
    "    #训练完模型后，可以运行该函数生成图片\n",
    "    g = generator_model()\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    g.load_weights('generator')\n",
    "    if nice:\n",
    "        d = discriminator_model()\n",
    "        d.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "        d.load_weights('discriminator')\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE*20, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        d_pret = d.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[1:3], dtype=np.float32)\n",
    "        nice_images = nice_images[:, :, :, None]\n",
    "        for i in range(BATCH_SIZE):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "        generated_images = g.predict(noise, verbose=0)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        \"./GAN/generated_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(BATCH_SIZE=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0\n",
      "Number of batches 1666\n",
      "36/36 [==============================] - 0s 12ms/step\n",
      "batch 0 d_loss : 0.682336\n",
      "batch 0 g_loss : 0.735015\n",
      "36/36 [==============================] - 0s 965us/step\n",
      "36/36 [==============================] - 0s 740us/step\n",
      "36/36 [==============================] - 0s 765us/step\n",
      "36/36 [==============================] - 0s 832us/step\n",
      "36/36 [==============================] - 0s 823us/step\n",
      "36/36 [==============================] - 0s 831us/step\n",
      "36/36 [==============================] - 0s 913us/step\n",
      "36/36 [==============================] - 0s 913us/step\n",
      "36/36 [==============================] - 0s 789us/step\n",
      "36/36 [==============================] - 0s 869us/step\n",
      "batch 10 d_loss : 0.393709\n",
      "batch 10 g_loss : 0.828054\n",
      "36/36 [==============================] - 0s 991us/step\n",
      "36/36 [==============================] - 0s 775us/step\n",
      "36/36 [==============================] - 0s 877us/step\n",
      "36/36 [==============================] - 0s 868us/step\n",
      "36/36 [==============================] - 0s 863us/step\n",
      "36/36 [==============================] - 0s 829us/step\n",
      "36/36 [==============================] - 0s 833us/step\n",
      "36/36 [==============================] - 0s 760us/step\n",
      "36/36 [==============================] - 0s 775us/step\n",
      "36/36 [==============================] - 0s 748us/step\n",
      "batch 20 d_loss : 0.211775\n",
      "batch 20 g_loss : 1.292953\n",
      "36/36 [==============================] - 0s 953us/step\n",
      "36/36 [==============================] - 0s 781us/step\n",
      "36/36 [==============================] - 0s 752us/step\n",
      "36/36 [==============================] - 0s 743us/step\n",
      "36/36 [==============================] - 0s 742us/step\n",
      "36/36 [==============================] - 0s 754us/step\n",
      "36/36 [==============================] - 0s 748us/step\n",
      "36/36 [==============================] - 0s 792us/step\n",
      "36/36 [==============================] - 0s 807us/step\n",
      "36/36 [==============================] - 0s 819us/step\n",
      "batch 30 d_loss : 0.094821\n",
      "batch 30 g_loss : 2.072156\n",
      "36/36 [==============================] - 0s 891us/step\n",
      "36/36 [==============================] - 0s 740us/step\n",
      "36/36 [==============================] - 0s 736us/step\n",
      "36/36 [==============================] - 0s 734us/step\n",
      "36/36 [==============================] - 0s 811us/step\n",
      "36/36 [==============================] - 0s 789us/step\n",
      "36/36 [==============================] - 0s 799us/step\n",
      "36/36 [==============================] - 0s 769us/step\n",
      "36/36 [==============================] - 0s 788us/step\n",
      "36/36 [==============================] - 0s 766us/step\n",
      "batch 40 d_loss : 0.068646\n",
      "batch 40 g_loss : 2.627500\n",
      "36/36 [==============================] - 0s 942us/step\n",
      "36/36 [==============================] - 0s 759us/step\n",
      "36/36 [==============================] - 0s 775us/step\n",
      "36/36 [==============================] - 0s 742us/step\n",
      "36/36 [==============================] - 0s 772us/step\n",
      "36/36 [==============================] - 0s 759us/step\n",
      "36/36 [==============================] - 0s 783us/step\n",
      "36/36 [==============================] - 0s 758us/step\n",
      "36/36 [==============================] - 0s 794us/step\n",
      "36/36 [==============================] - 0s 788us/step\n",
      "batch 50 d_loss : 0.069032\n",
      "batch 50 g_loss : 2.873433\n",
      "36/36 [==============================] - 0s 943us/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 781us/step\n",
      "36/36 [==============================] - 0s 829us/step\n",
      "36/36 [==============================] - 0s 807us/step\n",
      "36/36 [==============================] - 0s 769us/step\n",
      "36/36 [==============================] - 0s 758us/step\n",
      "36/36 [==============================] - 0s 792us/step\n",
      "36/36 [==============================] - 0s 763us/step\n",
      "36/36 [==============================] - 0s 812us/step\n",
      "batch 60 d_loss : 0.077229\n",
      "batch 60 g_loss : 2.920209\n",
      "36/36 [==============================] - 0s 907us/step\n",
      "36/36 [==============================] - 0s 739us/step\n",
      "36/36 [==============================] - 0s 790us/step\n",
      "36/36 [==============================] - 0s 746us/step\n",
      "36/36 [==============================] - 0s 733us/step\n",
      "36/36 [==============================] - 0s 768us/step\n",
      "36/36 [==============================] - 0s 885us/step\n",
      "36/36 [==============================] - 0s 767us/step\n",
      "36/36 [==============================] - 0s 741us/step\n",
      "36/36 [==============================] - 0s 756us/step\n",
      "batch 70 d_loss : 0.094962\n",
      "batch 70 g_loss : 2.775211\n",
      "36/36 [==============================] - 0s 923us/step\n",
      "36/36 [==============================] - 0s 744us/step\n",
      "36/36 [==============================] - 0s 744us/step\n",
      "36/36 [==============================] - 0s 781us/step\n",
      "36/36 [==============================] - 0s 762us/step\n",
      "36/36 [==============================] - 0s 730us/step\n",
      "36/36 [==============================] - 0s 733us/step\n",
      "36/36 [==============================] - 0s 752us/step\n",
      "36/36 [==============================] - 0s 748us/step\n",
      "36/36 [==============================] - 0s 738us/step\n",
      "batch 80 d_loss : 0.100059\n",
      "batch 80 g_loss : 2.908402\n",
      "36/36 [==============================] - 0s 913us/step\n",
      "36/36 [==============================] - 0s 969us/step\n",
      "36/36 [==============================] - 0s 913us/step\n",
      "36/36 [==============================] - 0s 801us/step\n",
      "36/36 [==============================] - 0s 794us/step\n",
      "36/36 [==============================] - 0s 757us/step\n",
      "36/36 [==============================] - 0s 864us/step\n",
      "36/36 [==============================] - 0s 719us/step\n",
      "36/36 [==============================] - 0s 786us/step\n",
      "36/36 [==============================] - 0s 898us/step\n",
      "batch 90 d_loss : 0.144778\n",
      "batch 90 g_loss : 2.694613\n",
      "36/36 [==============================] - 0s 942us/step\n",
      "36/36 [==============================] - 0s 892us/step\n",
      "36/36 [==============================] - 0s 727us/step\n",
      "36/36 [==============================] - 0s 773us/step\n",
      "36/36 [==============================] - 0s 756us/step\n",
      "36/36 [==============================] - 0s 725us/step\n",
      "36/36 [==============================] - 0s 886us/step\n",
      "36/36 [==============================] - 0s 883us/step\n",
      "36/36 [==============================] - 0s 814us/step\n",
      "36/36 [==============================] - 0s 809us/step\n",
      "batch 100 d_loss : 0.285740\n",
      "batch 100 g_loss : 2.110488\n",
      "36/36 [==============================] - 0s 948us/step\n",
      "36/36 [==============================] - 0s 926us/step\n",
      "36/36 [==============================] - 0s 784us/step\n",
      "36/36 [==============================] - 0s 776us/step\n",
      "36/36 [==============================] - 0s 770us/step\n",
      "36/36 [==============================] - 0s 729us/step\n",
      "36/36 [==============================] - 0s 785us/step\n",
      "36/36 [==============================] - 0s 795us/step\n",
      "36/36 [==============================] - 0s 863us/step\n",
      "36/36 [==============================] - 0s 835us/step\n",
      "batch 110 d_loss : 0.143972\n",
      "batch 110 g_loss : 1.958619\n",
      "36/36 [==============================] - 0s 903us/step\n",
      "36/36 [==============================] - 0s 741us/step\n",
      "36/36 [==============================] - 0s 736us/step\n",
      "36/36 [==============================] - 0s 872us/step\n",
      "36/36 [==============================] - 0s 889us/step\n",
      "36/36 [==============================] - 0s 843us/step\n",
      "36/36 [==============================] - 0s 848us/step\n",
      "36/36 [==============================] - 0s 947us/step\n",
      "36/36 [==============================] - 0s 742us/step\n",
      "36/36 [==============================] - 0s 819us/step\n",
      "batch 120 d_loss : 0.179341\n",
      "batch 120 g_loss : 2.017108\n",
      "36/36 [==============================] - 0s 978us/step\n",
      "36/36 [==============================] - 0s 746us/step\n",
      "36/36 [==============================] - 0s 747us/step\n",
      "36/36 [==============================] - 0s 756us/step\n",
      "36/36 [==============================] - 0s 724us/step\n",
      "36/36 [==============================] - 0s 812us/step\n",
      "36/36 [==============================] - 0s 756us/step\n",
      "36/36 [==============================] - 0s 772us/step\n",
      "36/36 [==============================] - 0s 760us/step\n",
      "36/36 [==============================] - 0s 829us/step\n",
      "batch 130 d_loss : 0.176262\n",
      "batch 130 g_loss : 2.389704\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 736us/step\n",
      "36/36 [==============================] - 0s 755us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 746us/step\n",
      "36/36 [==============================] - 0s 808us/step\n",
      "36/36 [==============================] - 0s 990us/step\n",
      "36/36 [==============================] - 0s 746us/step\n",
      "36/36 [==============================] - 0s 746us/step\n",
      "36/36 [==============================] - 0s 754us/step\n",
      "36/36 [==============================] - 0s 719us/step\n",
      "batch 140 d_loss : 0.111333\n",
      "batch 140 g_loss : 2.630879\n",
      "36/36 [==============================] - 0s 977us/step\n",
      "36/36 [==============================] - 0s 739us/step\n",
      "36/36 [==============================] - 0s 737us/step\n",
      "36/36 [==============================] - 0s 777us/step\n",
      "36/36 [==============================] - 0s 777us/step\n",
      "36/36 [==============================] - 0s 787us/step\n",
      "36/36 [==============================] - 0s 781us/step\n",
      "36/36 [==============================] - 0s 780us/step\n",
      "36/36 [==============================] - 0s 830us/step\n",
      "36/36 [==============================] - 0s 831us/step\n",
      "batch 150 d_loss : 0.108486\n",
      "batch 150 g_loss : 3.155914\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 766us/step\n",
      "36/36 [==============================] - 0s 733us/step\n",
      "36/36 [==============================] - 0s 790us/step\n",
      "36/36 [==============================] - 0s 788us/step\n",
      "36/36 [==============================] - 0s 743us/step\n",
      "36/36 [==============================] - 0s 765us/step\n",
      "36/36 [==============================] - 0s 741us/step\n",
      "36/36 [==============================] - 0s 777us/step\n",
      "36/36 [==============================] - 0s 812us/step\n",
      "batch 160 d_loss : 0.079475\n",
      "batch 160 g_loss : 3.474900\n",
      "36/36 [==============================] - 0s 955us/step\n",
      "36/36 [==============================] - 0s 843us/step\n",
      "36/36 [==============================] - 0s 794us/step\n",
      "36/36 [==============================] - 0s 850us/step\n",
      "36/36 [==============================] - 0s 949us/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 832us/step\n",
      "36/36 [==============================] - 0s 836us/step\n",
      "36/36 [==============================] - 0s 789us/step\n",
      "36/36 [==============================] - 0s 770us/step\n",
      "batch 170 d_loss : 0.038735\n",
      "batch 170 g_loss : 3.724096\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 829us/step\n",
      "36/36 [==============================] - 0s 802us/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 856us/step\n",
      "36/36 [==============================] - 0s 821us/step\n",
      "36/36 [==============================] - 0s 853us/step\n",
      "batch 180 d_loss : 0.096762\n",
      "batch 180 g_loss : 4.000908\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 830us/step\n",
      "36/36 [==============================] - 0s 917us/step\n",
      "36/36 [==============================] - 0s 878us/step\n",
      "36/36 [==============================] - 0s 910us/step\n",
      "36/36 [==============================] - 0s 806us/step\n",
      "36/36 [==============================] - 0s 790us/step\n",
      "36/36 [==============================] - 0s 743us/step\n",
      "36/36 [==============================] - 0s 847us/step\n",
      "36/36 [==============================] - 0s 850us/step\n",
      "batch 190 d_loss : 0.101024\n",
      "batch 190 g_loss : 3.884231\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 889us/step\n",
      "36/36 [==============================] - 0s 867us/step\n",
      "36/36 [==============================] - 0s 791us/step\n",
      "36/36 [==============================] - 0s 867us/step\n",
      "36/36 [==============================] - 0s 851us/step\n",
      "36/36 [==============================] - 0s 793us/step\n",
      "36/36 [==============================] - 0s 853us/step\n",
      "36/36 [==============================] - 0s 928us/step\n",
      "36/36 [==============================] - 0s 865us/step\n",
      "batch 200 d_loss : 0.099127\n",
      "batch 200 g_loss : 4.090193\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 819us/step\n",
      "36/36 [==============================] - 0s 808us/step\n",
      "36/36 [==============================] - 0s 753us/step\n",
      "36/36 [==============================] - 0s 790us/step\n",
      "36/36 [==============================] - 0s 776us/step\n",
      "36/36 [==============================] - 0s 752us/step\n",
      "36/36 [==============================] - 0s 757us/step\n",
      "36/36 [==============================] - 0s 804us/step\n",
      "36/36 [==============================] - 0s 784us/step\n",
      "batch 210 d_loss : 0.068449\n",
      "batch 210 g_loss : 4.486899\n",
      "36/36 [==============================] - 0s 938us/step\n",
      "36/36 [==============================] - 0s 770us/step\n",
      "36/36 [==============================] - 0s 774us/step\n",
      "36/36 [==============================] - 0s 798us/step\n",
      "36/36 [==============================] - 0s 785us/step\n",
      "36/36 [==============================] - 0s 806us/step\n",
      "36/36 [==============================] - 0s 798us/step\n",
      "36/36 [==============================] - 0s 826us/step\n",
      "36/36 [==============================] - 0s 852us/step\n",
      "36/36 [==============================] - 0s 789us/step\n",
      "batch 220 d_loss : 0.122193\n",
      "batch 220 g_loss : 5.272979\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 889us/step\n",
      "36/36 [==============================] - 0s 935us/step\n",
      "36/36 [==============================] - 0s 881us/step\n",
      "36/36 [==============================] - 0s 834us/step\n",
      "36/36 [==============================] - 0s 813us/step\n",
      "36/36 [==============================] - 0s 780us/step\n",
      "36/36 [==============================] - 0s 763us/step\n",
      "36/36 [==============================] - 0s 750us/step\n",
      "36/36 [==============================] - 0s 785us/step\n",
      "batch 230 d_loss : 0.064604\n",
      "batch 230 g_loss : 4.764647\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 807us/step\n",
      "36/36 [==============================] - 0s 759us/step\n",
      "36/36 [==============================] - 0s 904us/step\n",
      "36/36 [==============================] - 0s 919us/step\n",
      "36/36 [==============================] - 0s 907us/step\n",
      "36/36 [==============================] - 0s 758us/step\n",
      "36/36 [==============================] - 0s 794us/step\n",
      "36/36 [==============================] - 0s 764us/step\n",
      "36/36 [==============================] - 0s 791us/step\n",
      "batch 240 d_loss : 0.129068\n",
      "batch 240 g_loss : 4.745943\n",
      "36/36 [==============================] - 0s 980us/step\n",
      "36/36 [==============================] - 0s 760us/step\n",
      "36/36 [==============================] - 0s 792us/step\n",
      "36/36 [==============================] - 0s 819us/step\n",
      "36/36 [==============================] - 0s 807us/step\n",
      "36/36 [==============================] - 0s 841us/step\n",
      "36/36 [==============================] - 0s 869us/step\n",
      "36/36 [==============================] - 0s 867us/step\n",
      "36/36 [==============================] - 0s 938us/step\n",
      "36/36 [==============================] - 0s 880us/step\n",
      "batch 250 d_loss : 0.207131\n",
      "batch 250 g_loss : 4.334843\n",
      "36/36 [==============================] - 0s 977us/step\n",
      "36/36 [==============================] - 0s 768us/step\n",
      "36/36 [==============================] - 0s 780us/step\n",
      "36/36 [==============================] - 0s 772us/step\n",
      "36/36 [==============================] - 0s 803us/step\n",
      "36/36 [==============================] - 0s 810us/step\n",
      "36/36 [==============================] - 0s 755us/step\n",
      "36/36 [==============================] - 0s 798us/step\n",
      "36/36 [==============================] - 0s 798us/step\n",
      "36/36 [==============================] - 0s 803us/step\n",
      "batch 260 d_loss : 0.203845\n",
      "batch 260 g_loss : 4.184690\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 781us/step\n",
      "36/36 [==============================] - 0s 789us/step\n",
      "36/36 [==============================] - 0s 799us/step\n",
      "36/36 [==============================] - 0s 774us/step\n",
      "36/36 [==============================] - 0s 785us/step\n",
      "36/36 [==============================] - 0s 797us/step\n",
      "36/36 [==============================] - 0s 770us/step\n",
      "36/36 [==============================] - 0s 808us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 748us/step\n",
      "batch 270 d_loss : 0.210647\n",
      "batch 270 g_loss : 3.074482\n",
      "36/36 [==============================] - 0s 928us/step\n",
      "36/36 [==============================] - 0s 818us/step\n",
      "36/36 [==============================] - 0s 777us/step\n",
      "36/36 [==============================] - 0s 770us/step\n",
      "36/36 [==============================] - 0s 754us/step\n",
      "36/36 [==============================] - 0s 792us/step\n",
      "36/36 [==============================] - 0s 738us/step\n",
      "36/36 [==============================] - 0s 747us/step\n",
      "36/36 [==============================] - 0s 748us/step\n",
      "36/36 [==============================] - 0s 760us/step\n",
      "batch 280 d_loss : 0.246642\n",
      "batch 280 g_loss : 2.662836\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 879us/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 935us/step\n",
      "36/36 [==============================] - 0s 864us/step\n",
      "36/36 [==============================] - 0s 823us/step\n",
      "36/36 [==============================] - 0s 817us/step\n",
      "36/36 [==============================] - 0s 841us/step\n",
      "36/36 [==============================] - 0s 794us/step\n",
      "batch 290 d_loss : 0.264559\n",
      "batch 290 g_loss : 2.368301\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 813us/step\n",
      "36/36 [==============================] - 0s 837us/step\n",
      "36/36 [==============================] - 0s 771us/step\n",
      "36/36 [==============================] - 0s 808us/step\n",
      "36/36 [==============================] - 0s 801us/step\n",
      "36/36 [==============================] - 0s 819us/step\n",
      "36/36 [==============================] - 0s 749us/step\n",
      "36/36 [==============================] - 0s 773us/step\n",
      "36/36 [==============================] - 0s 746us/step\n",
      "batch 300 d_loss : 0.295778\n",
      "batch 300 g_loss : 1.837642\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 786us/step\n",
      "36/36 [==============================] - 0s 765us/step\n",
      "36/36 [==============================] - 0s 761us/step\n",
      "36/36 [==============================] - 0s 769us/step\n",
      "36/36 [==============================] - 0s 738us/step\n",
      "36/36 [==============================] - 0s 792us/step\n",
      "36/36 [==============================] - 0s 765us/step\n",
      "36/36 [==============================] - 0s 804us/step\n",
      "36/36 [==============================] - 0s 774us/step\n",
      "batch 310 d_loss : 0.227886\n",
      "batch 310 g_loss : 2.054160\n",
      "36/36 [==============================] - 0s 973us/step\n",
      "36/36 [==============================] - 0s 751us/step\n",
      "36/36 [==============================] - 0s 809us/step\n",
      "36/36 [==============================] - 0s 755us/step\n",
      "36/36 [==============================] - 0s 813us/step\n",
      "36/36 [==============================] - 0s 816us/step\n",
      "36/36 [==============================] - 0s 854us/step\n",
      "36/36 [==============================] - 0s 821us/step\n",
      "36/36 [==============================] - 0s 877us/step\n",
      "36/36 [==============================] - 0s 817us/step\n",
      "batch 320 d_loss : 0.244912\n",
      "batch 320 g_loss : 1.976597\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 838us/step\n",
      "36/36 [==============================] - 0s 824us/step\n",
      "36/36 [==============================] - 0s 795us/step\n",
      "36/36 [==============================] - 0s 811us/step\n",
      "36/36 [==============================] - 0s 815us/step\n",
      "36/36 [==============================] - 0s 868us/step\n",
      "36/36 [==============================] - 0s 818us/step\n",
      "36/36 [==============================] - 0s 791us/step\n",
      "36/36 [==============================] - 0s 855us/step\n",
      "batch 330 d_loss : 0.194289\n",
      "batch 330 g_loss : 2.075638\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 832us/step\n",
      "36/36 [==============================] - 0s 784us/step\n",
      "36/36 [==============================] - 0s 813us/step\n",
      "36/36 [==============================] - 0s 812us/step\n",
      "36/36 [==============================] - 0s 859us/step\n",
      "36/36 [==============================] - 0s 755us/step\n",
      "36/36 [==============================] - 0s 758us/step\n",
      "36/36 [==============================] - 0s 768us/step\n",
      "36/36 [==============================] - 0s 781us/step\n",
      "batch 340 d_loss : 0.206124\n",
      "batch 340 g_loss : 1.966317\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 909us/step\n",
      "36/36 [==============================] - 0s 828us/step\n",
      "36/36 [==============================] - 0s 798us/step\n",
      "36/36 [==============================] - 0s 829us/step\n",
      "36/36 [==============================] - 0s 768us/step\n",
      "36/36 [==============================] - 0s 800us/step\n",
      "36/36 [==============================] - 0s 806us/step\n",
      "36/36 [==============================] - 0s 760us/step\n",
      "36/36 [==============================] - 0s 786us/step\n",
      "batch 350 d_loss : 0.154631\n",
      "batch 350 g_loss : 2.441333\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 795us/step\n",
      "36/36 [==============================] - 0s 813us/step\n",
      "36/36 [==============================] - 0s 815us/step\n",
      "36/36 [==============================] - 0s 775us/step\n",
      "36/36 [==============================] - 0s 823us/step\n",
      "36/36 [==============================] - 0s 770us/step\n",
      "36/36 [==============================] - 0s 795us/step\n",
      "36/36 [==============================] - 0s 764us/step\n",
      "36/36 [==============================] - 0s 762us/step\n",
      "batch 360 d_loss : 0.228697\n",
      "batch 360 g_loss : 2.243107\n",
      "36/36 [==============================] - 0s 999us/step\n",
      "36/36 [==============================] - 0s 800us/step\n",
      "36/36 [==============================] - 0s 796us/step\n",
      "36/36 [==============================] - 0s 848us/step\n",
      "36/36 [==============================] - 0s 753us/step\n",
      "36/36 [==============================] - 0s 813us/step\n",
      "36/36 [==============================] - 0s 820us/step\n",
      "36/36 [==============================] - 0s 810us/step\n",
      "36/36 [==============================] - 0s 792us/step\n",
      "36/36 [==============================] - 0s 772us/step\n",
      "batch 370 d_loss : 0.168863\n",
      "batch 370 g_loss : 2.329559\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 725us/step\n",
      "36/36 [==============================] - 0s 777us/step\n",
      "36/36 [==============================] - 0s 749us/step\n",
      "36/36 [==============================] - 0s 734us/step\n",
      "36/36 [==============================] - 0s 750us/step\n",
      "36/36 [==============================] - 0s 812us/step\n",
      "36/36 [==============================] - 0s 800us/step\n",
      "36/36 [==============================] - 0s 787us/step\n",
      "36/36 [==============================] - 0s 821us/step\n",
      "batch 380 d_loss : 0.190576\n",
      "batch 380 g_loss : 2.103904\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 768us/step\n",
      "36/36 [==============================] - 0s 839us/step\n",
      "36/36 [==============================] - 0s 843us/step\n",
      "36/36 [==============================] - 0s 779us/step\n",
      "36/36 [==============================] - 0s 751us/step\n",
      "36/36 [==============================] - 0s 804us/step\n",
      "36/36 [==============================] - 0s 783us/step\n",
      "36/36 [==============================] - 0s 743us/step\n",
      "36/36 [==============================] - 0s 771us/step\n",
      "batch 390 d_loss : 0.236182\n",
      "batch 390 g_loss : 2.252042\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 780us/step\n",
      "36/36 [==============================] - 0s 763us/step\n",
      "36/36 [==============================] - 0s 763us/step\n",
      "36/36 [==============================] - 0s 755us/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 759us/step\n",
      "36/36 [==============================] - 0s 755us/step\n",
      "36/36 [==============================] - 0s 795us/step\n",
      "batch 400 d_loss : 0.122738\n",
      "batch 400 g_loss : 2.619785\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 866us/step\n",
      "36/36 [==============================] - 0s 951us/step\n",
      "36/36 [==============================] - 0s 944us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 821us/step\n",
      "36/36 [==============================] - 0s 785us/step\n",
      "36/36 [==============================] - 0s 828us/step\n",
      "36/36 [==============================] - 0s 808us/step\n",
      "36/36 [==============================] - 0s 786us/step\n",
      "36/36 [==============================] - 0s 792us/step\n",
      "batch 410 d_loss : 0.197778\n",
      "batch 410 g_loss : 2.221021\n",
      "36/36 [==============================] - 0s 998us/step\n",
      "36/36 [==============================] - 0s 810us/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 806us/step\n",
      "36/36 [==============================] - 0s 882us/step\n",
      "36/36 [==============================] - 0s 847us/step\n",
      "36/36 [==============================] - 0s 836us/step\n",
      "36/36 [==============================] - 0s 809us/step\n",
      "36/36 [==============================] - 0s 821us/step\n",
      "36/36 [==============================] - 0s 795us/step\n",
      "batch 420 d_loss : 0.123592\n",
      "batch 420 g_loss : 2.858892\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 811us/step\n",
      "36/36 [==============================] - 0s 806us/step\n",
      "36/36 [==============================] - 0s 805us/step\n",
      "36/36 [==============================] - 0s 805us/step\n",
      "36/36 [==============================] - 0s 787us/step\n",
      "36/36 [==============================] - 0s 753us/step\n",
      "36/36 [==============================] - 0s 786us/step\n",
      "36/36 [==============================] - 0s 877us/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "batch 430 d_loss : 0.156469\n",
      "batch 430 g_loss : 2.657044\n",
      "36/36 [==============================] - 0s 926us/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 777us/step\n",
      "36/36 [==============================] - 0s 793us/step\n",
      "36/36 [==============================] - 0s 808us/step\n",
      "36/36 [==============================] - 0s 838us/step\n",
      "36/36 [==============================] - 0s 900us/step\n",
      "36/36 [==============================] - 0s 786us/step\n",
      "36/36 [==============================] - 0s 839us/step\n",
      "36/36 [==============================] - 0s 796us/step\n",
      "batch 440 d_loss : 0.114704\n",
      "batch 440 g_loss : 2.612553\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 817us/step\n",
      "36/36 [==============================] - 0s 871us/step\n",
      "36/36 [==============================] - 0s 776us/step\n",
      "36/36 [==============================] - 0s 778us/step\n",
      "36/36 [==============================] - 0s 760us/step\n",
      "36/36 [==============================] - 0s 755us/step\n",
      "36/36 [==============================] - 0s 775us/step\n",
      "36/36 [==============================] - 0s 808us/step\n",
      "36/36 [==============================] - 0s 859us/step\n",
      "batch 450 d_loss : 0.115809\n",
      "batch 450 g_loss : 2.589263\n",
      "36/36 [==============================] - 0s 996us/step\n",
      "36/36 [==============================] - 0s 749us/step\n",
      "36/36 [==============================] - 0s 806us/step\n",
      "36/36 [==============================] - 0s 817us/step\n",
      "36/36 [==============================] - 0s 862us/step\n",
      "36/36 [==============================] - 0s 741us/step\n",
      "36/36 [==============================] - 0s 774us/step\n",
      "36/36 [==============================] - 0s 972us/step\n",
      "36/36 [==============================] - 0s 942us/step\n",
      "36/36 [==============================] - 0s 775us/step\n",
      "batch 460 d_loss : 0.101362\n",
      "batch 460 g_loss : 2.988387\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 902us/step\n",
      "36/36 [==============================] - 0s 784us/step\n",
      "36/36 [==============================] - 0s 742us/step\n",
      "36/36 [==============================] - 0s 860us/step\n",
      "36/36 [==============================] - 0s 809us/step\n",
      "36/36 [==============================] - 0s 807us/step\n",
      "36/36 [==============================] - 0s 780us/step\n",
      "36/36 [==============================] - 0s 788us/step\n",
      "36/36 [==============================] - 0s 805us/step\n",
      "batch 470 d_loss : 0.103979\n",
      "batch 470 g_loss : 2.722085\n",
      "36/36 [==============================] - 0s 996us/step\n",
      "36/36 [==============================] - 0s 980us/step\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 796us/step\n",
      "36/36 [==============================] - 0s 844us/step\n",
      "36/36 [==============================] - 0s 820us/step\n",
      "36/36 [==============================] - 0s 784us/step\n",
      "36/36 [==============================] - 0s 770us/step\n",
      "36/36 [==============================] - 0s 821us/step\n",
      "36/36 [==============================] - 0s 791us/step\n",
      "batch 480 d_loss : 0.105648\n",
      "batch 480 g_loss : 2.730220\n",
      "36/36 [==============================] - 0s 936us/step\n",
      "36/36 [==============================] - 0s 796us/step\n",
      "36/36 [==============================] - 0s 797us/step\n",
      "36/36 [==============================] - 0s 771us/step\n",
      "36/36 [==============================] - 0s 754us/step\n",
      "36/36 [==============================] - 0s 769us/step\n",
      "36/36 [==============================] - 0s 745us/step\n",
      "36/36 [==============================] - 0s 796us/step\n",
      "36/36 [==============================] - 0s 743us/step\n",
      "36/36 [==============================] - 0s 766us/step\n",
      "batch 490 d_loss : 0.142068\n",
      "batch 490 g_loss : 2.897335\n",
      "36/36 [==============================] - 0s 981us/step\n",
      "36/36 [==============================] - 0s 784us/step\n",
      "36/36 [==============================] - 0s 802us/step\n",
      "36/36 [==============================] - 0s 806us/step\n",
      "36/36 [==============================] - 0s 749us/step\n",
      "36/36 [==============================] - 0s 828us/step\n",
      "36/36 [==============================] - 0s 773us/step\n",
      "36/36 [==============================] - 0s 795us/step\n",
      "36/36 [==============================] - 0s 759us/step\n",
      "36/36 [==============================] - 0s 803us/step\n",
      "batch 500 d_loss : 0.077329\n",
      "batch 500 g_loss : 3.360003\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "36/36 [==============================] - 0s 829us/step\n",
      "36/36 [==============================] - 0s 844us/step\n",
      "36/36 [==============================] - 0s 829us/step\n",
      "36/36 [==============================] - 0s 785us/step\n",
      "36/36 [==============================] - 0s 802us/step\n",
      "36/36 [==============================] - 0s 783us/step\n",
      "36/36 [==============================] - 0s 760us/step\n",
      "36/36 [==============================] - 0s 828us/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-fd51cae57762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-ada3964322d9>\u001b[0m in \u001b[0;36mtrain_k\u001b[0;34m(BATCH_SIZE, k)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m#判别器的损失；在一个batch的数据上进行一次参数更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;31m#print(\"batch %d d_loss : %f\" % (index, d_loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_k(BATCH_SIZE=36,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(132)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda Py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
